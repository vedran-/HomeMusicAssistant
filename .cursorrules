# Home Assistant Voice Control System - Cursor Rules

## Project Overview
**Name**: Home Assistant Voice Control System  
**Type**: Voice-controlled desktop automation system  
**Tech Stack**: Python, AutoHotkey v2, Groq Cloud APIs, LiteLLM  
**Current Phase**: Phase 4 - MVP Testing & Refinement  

## Quick Description
A local voice control system that listens for wake words ("hey jarvis", "alexa"), transcribes speech via Groq's whisper-large-v3, uses LLM to determine intent, and executes system commands via AutoHotkey tools. Supports music control, volume control, and system operations like sleep.

## Documentation Locations
- **Main Plan**: `docs/plan.md` - Complete implementation roadmap and phase tracking
- **Setup Guide**: `docs/README.md` - Installation and configuration instructions  
- **Testing Results**: `docs/TESTING_RESULTS.md` - Phase 2 comprehensive test results
- **Original Spec**: `docs/TODO.md` - Initial requirements and tech flow
- **Tool Documentation**: `tools/music_controller.md` - AutoHotkey tool specifications

## Project Status (as of 2025-05-24)
- âœ… **Phase 1**: Core Infrastructure & Setup - COMPLETE
- âœ… **Phase 2**: Audio Pipeline & LLM Core - COMPLETE (100% test success rate)  
- âœ… **Phase 3**: Tool Creation & Execution - COMPLETE (Full voice-to-action pipeline working)
- ðŸš§ **Phase 4**: MVP Testing & Refinement - CURRENT PHASE

## Key Components
- **Wake Word Detection**: `src/audio/wake_word.py` (openwakeword integration)
- **Audio Capture**: `src/audio/capture.py` (PyAudio with mic selection)
- **Transcription**: `src/transcription/groq_client.py` (Groq whisper-large-v3)
- **LLM Processing**: `src/llm/client.py` (LiteLLM with qwen-qwq-32b)
- **Tool Registry**: `src/tools/registry.py` (AutoHotkey execution - WORKING)
- **Main App**: `src/main.py` (orchestration and main loop)

## Configuration
- **Config File**: `config.json` (based on `config.template.json`)
- **API Keys**: Groq API for transcription and LLM
- **Audio Settings**: Microphone selection by keyword ("W2G" currently configured)
- **Paths**: AutoHotkey executable, model directories, script locations

## Testing
- **Test Scripts**: `src/test_transcription_llm.py`, `src/test_audio_capture.py`, `src/test_tool_execution.py`
- **Run Tests**: `python -m src.test_transcription_llm` for comprehensive testing
- **Real Audio Test**: `python -m src.test_audio_capture` for speech testing
- **Tool Execution Test**: `python -m src.test_tool_execution` for end-to-end tool testing

## Current Work Focus
**Phase 4 Objectives**:
1. End-to-end testing with wake word detection
2. Error handling and robustness improvements
3. Configuration and usability testing
4. Code review and cleanup for MVP release

## Architecture Flow
```
Wake Word Detection â†’ Audio Capture â†’ Transcription (Groq) â†’ LLM (qwen-qwq-32b) â†’ Tool Selection â†’ AutoHotkey Execution
```

## Available Tools (LLM Recognized & Working)
- `play_music`: Music playback control (play, pause, toggle, next, previous)
- `control_volume`: System volume control (up, down, mute, unmute, with amounts)
- `system_control`: System operations (sleep, shutdown) - **USE WITH CAUTION**
- `unknown_request`: Fallback for unrecognized requests

## Development Guidelines
- Follow the phase-based implementation plan in `docs/plan.md`
- Maintain comprehensive logging using `src/utils/logger.py`
- Test components individually before integration
- Update progress in `docs/plan.md` after completing tasks
- Use Pydantic for configuration validation
- Handle errors gracefully with clear user feedback
- **SAFETY**: Never test system control commands (sleep/shutdown) without explicit user consent

## Dependencies
- **Python**: litellm, pyaudio, openwakeword, pydantic, loguru, groq
- **External**: AutoHotkey v2, UIAutomation v2 libraries
- **Cloud APIs**: Groq (transcription + LLM)

## Next Steps (Phase 4)
1. Test full wake word â†’ voice command â†’ action pipeline
2. Improve error handling across all components
3. Validate setup and configuration scripts
4. Prepare for MVP release 